{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRMIAUevVwSEv+5Wys7g2K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tobypullan/tobypullan.github.io/blob/main/MicrogradPart1Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "S7eHuQsUTGXb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1 Exercises:\n",
        "\n",
        "# Exercise 1: Derivative of a Simple Function with One Input\n",
        "\n",
        "# Let's explore how to numerically approximate the derivative of a function.\n",
        "\n",
        "\n",
        "def f(x):\n",
        "    \"\"\"A simple quadratic function.\"\"\"\n",
        "    return x**2 + 3*x + 1\n",
        "\n",
        "def plot_function(f, x_range):\n",
        "    \"\"\"Plot the function and its derivative.\"\"\"\n",
        "    x = np.linspace(x_range[0], x_range[1], 100)\n",
        "    y = f(x)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(x, y, label='f(x)')\n",
        "    plt.title('Function')\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot the function\n",
        "plot_function(f, (-5, 5))"
      ],
      "metadata": {
        "id": "pq3op9DQxK_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement a function to numerically approximate the derivative\n",
        "def approximate_derivative(f, x, h=1e-5):\n",
        "    \"\"\"\n",
        "    Approximate the derivative of function f at point x.\n",
        "\n",
        "    Parameters:\n",
        "    f (function): The function to differentiate\n",
        "    x (float): The point at which to calculate the derivative\n",
        "    h (float): The small step size for approximation\n",
        "\n",
        "    Returns:\n",
        "    float: The approximated derivative\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "    pass\n",
        "\n",
        "# Test your implementation\n",
        "x_test = 2\n",
        "approx_deriv = approximate_derivative(f, x_test)\n",
        "print(f\"Approximated derivative of f(x) at x = {x_test}: {approx_deriv}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GO-MGQYvTUH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bonus: Can you think of a way to verify if your approximation is correct?\n",
        "# Hint: For this simple function, you can calculate the actual derivative analytically.\n",
        "\n",
        "# Exercise 2: Exploring the Effect of Step Size\n",
        "\n",
        "def plot_approximation_error(f, x, true_derivative):\n",
        "    \"\"\"Plot the approximation error for different step sizes.\"\"\"\n",
        "    step_sizes = np.logspace(-12, 0, 100)\n",
        "    errors = [abs(approximate_derivative(f, x, h) - true_derivative) for h in step_sizes]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.loglog(step_sizes, errors)\n",
        "    plt.title('Approximation Error vs Step Size')\n",
        "    plt.xlabel('Step Size (h)')\n",
        "    plt.ylabel('Absolute Error')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# TODO: Calculate the true derivative of f(x) at x = 2\n",
        "true_derivative = None  # Replace None with the correct value\n",
        "\n",
        "# Plot the approximation error\n",
        "plot_approximation_error(f, 2, true_derivative)\n"
      ],
      "metadata": {
        "id": "UKqLHPfkxAWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Questions to consider:\n",
        "# 1. What happens to the approximation error as the step size becomes very small?\n",
        "# 2. What happens as the step size becomes very large?\n",
        "# 3. Is there an optimal step size? Why or why not?"
      ],
      "metadata": {
        "id": "cSACz6KsxF-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: Derivatives of Functions with Multiple Inputs\n",
        "\n",
        "def f(x, y, z):\n",
        "    \"\"\"A function with multiple inputs.\"\"\"\n",
        "    return x**2 + y*z + np.sin(x*y)"
      ],
      "metadata": {
        "id": "EHdhYSmIxZz_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def partial_derivative(f, var, point, h=1e-5):\n",
        "    \"\"\"\n",
        "    Calculate the partial derivative of a function with respect to a specific variable.\n",
        "\n",
        "    Parameters:\n",
        "    f (function): The function to differentiate\n",
        "    var (int): The index of the variable to differentiate with respect to\n",
        "    point (tuple): The point at which to calculate the derivative\n",
        "    h (float): The small step size for approximation\n",
        "\n",
        "    Returns:\n",
        "    float: The approximated partial derivative\n",
        "    \"\"\"\n",
        "    # TODO: Implement the partial derivative function\n",
        "    point_list = list(point)\n",
        "    point_list[var] += h\n",
        "    return (f(*point_list) - f(*point)) / h\n",
        "# Test the partial derivative function\n",
        "test_point = (1, 2, 3)\n",
        "print(f\"Partial derivative with respect to x at {test_point}: {partial_derivative(f, 0, test_point)}\")\n",
        "print(f\"Partial derivative with respect to y at {test_point}: {partial_derivative(f, 1, test_point)}\")\n",
        "print(f\"Partial derivative with respect to z at {test_point}: {partial_derivative(f, 2, test_point)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9Oeuj3VTaya",
        "outputId": "f233bccc-9e6c-43c9-ba2c-34b268f4bf99"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partial derivative with respect to x at (1, 2, 3): 1.1676981410246867\n",
            "Partial derivative with respect to y at (1, 2, 3): 2.5838486170215447\n",
            "Partial derivative with respect to z at (1, 2, 3): 2.0000000000131024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# My partial derivative function:\n",
        "# point_list = list(point)\n",
        "# point_list[var] += h\n",
        "# return (f(*point_list) - f(*point)) / h\n",
        "\n",
        "# Explanation:\n",
        "# Line 1: converts the tuple of points into a list to enable h to be added to\n",
        "# each point\n",
        "# Line 2: adds h to the point at var\n",
        "# Line 3: unpacks points in point_list and point. calculates partial derivative\n",
        "# with respect to the point at position var in the point_list and returns this."
      ],
      "metadata": {
        "id": "gschLCn8xIKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collecting the partial derivatives\n",
        "def gradient(f, point, h=1e-5):\n",
        "    \"\"\"\n",
        "    Calculate the gradient of a function at a given point.\n",
        "\n",
        "    Parameters:\n",
        "    f (function): The function to differentiate\n",
        "    point (tuple): The point at which to calculate the gradient\n",
        "    h (float): The small step size for approximation\n",
        "\n",
        "    Returns:\n",
        "    numpy.array: The gradient vector\n",
        "    \"\"\"\n",
        "    return np.array([partial_derivative(f, i, point, h) for i in range(len(point))])\n",
        "\n",
        "# Test the gradient function\n",
        "grad = gradient(f, test_point)\n",
        "print(f\"Gradient at {test_point}: {grad}\")\n",
        "\n",
        "# Questions to consider:\n",
        "# 1. How does the concept of partial derivatives relate to the gradient of the loss function?\n",
        "# 2. Can you think of a real-world scenario where calculating gradients of multi-input functions is useful?\n",
        "# 3. How might you verify the correctness of your gradient calculation?\n"
      ],
      "metadata": {
        "id": "8bQDH4MaxrQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answers to Questions (Exercise 1):\n",
        "\n",
        "## 1. What happens to the approximation error as the step size becomes very small?\n",
        "As the step size becomes very small, the approximation error initially decreases,\n",
        "but then starts to increase again due to numerical precision limitations of floating-point arithmetic.\n",
        "This is known as round-off error.\n",
        "\n",
        "## 2. What happens as the step size becomes very large?\n",
        "As the step size becomes very large, the approximation error increases.\n",
        "This is because the approximation becomes less accurate as we move further away from the point of interest.\n",
        "\n",
        "## 3. Is there an optimal step size? Why or why not?\n",
        "Yes, there is typically an optimal step size. It's a balance between two competing factors:\n",
        "- Too small: leads to round-off errors due to limited floating-point precision\n",
        "- Too large: leads to poor approximation of the tangent line\n",
        "\n",
        "The optimal step size minimizes the combined effect of these two sources of error.\n",
        "It often occurs where the curve in the log-log plot of error vs. step size has a minimum.\n"
      ],
      "metadata": {
        "id": "KLvfXr2PTe6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answers to Questions to Consider (Exercise 2):\n",
        "\n",
        "## 1. How does the concept of partial derivatives relate to the gradient of the loss function?\n",
        "\n",
        "The gradient is a vector composed of all the partial derivatives of a function with respect to each of its input variables. Each component of the gradient represents how the function changes when we vary one input variable while keeping all others constant. In essence, the gradient is a collection of all partial derivatives, providing a complete picture of how the function changes with respect to all its inputs.\n",
        "\n",
        "## 2. Can you think of a real-world scenario where calculating gradients of multi-input functions is useful?\n",
        "\n",
        "One practical application is in machine learning, specifically in training neural networks through gradient descent. The gradient of the loss function with respect to the model parameters indicates the direction of steepest increase in the loss. By moving in the opposite direction of this gradient, we can adjust the model parameters to minimize the loss and improve the model's performance.\n",
        "\n",
        "Another example is in optimization problems in economics or engineering, where we might want to maximize or minimize a function that depends on multiple variables.\n",
        "\n",
        "## 3. How might you verify the correctness of your gradient calculation?\n",
        "\n",
        "There are several ways to verify the gradient calculation:\n",
        "\n",
        "a) Compare with analytical derivatives: For simple functions, we can calculate the gradient analytically and compare it with our numerical approximation.\n",
        "\n",
        "b) Finite difference method: We can use a more accurate finite difference method (like central difference) and compare it with our current implementation.\n",
        "\n",
        "c) Gradient checking: We can slightly perturb each input variable and observe if the function's change matches what we'd expect based on our calculated gradient.\n",
        "\n",
        "d) Symmetry tests: For some functions, we might know that certain partial derivatives should be equal due to the function's symmetry.\n",
        "\n",
        "e) Unit tests: We can create test cases with known gradients and ensure our function produces the expected results.\n"
      ],
      "metadata": {
        "id": "7-gMuHTpTkYI"
      }
    }
  ]
}