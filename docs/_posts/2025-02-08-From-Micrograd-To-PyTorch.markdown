---
layout: post
title:  "From Micrograd to PyTorch (and building a neural network)"
date:   2025-02-08 21:41:26 +0000
categories: Andrej Karpathy | Neural Networks Zero To Hero
usemathjax: true
---
## Introduction

In this post we will compare the PyTorch and Micrograd APIs, build a small neural network and train it using a small dataset.

## A comparison to PyTorch

## Creating a tiny dataset and writing the loss function

## Collecting all the parameters of the neural net

## Optimising the network manually

## Summary of what we learned and how to go towards modern neural nets

## How PyTorch implements tanh

## Conclusion
doing the same thing but in PyTorch: comparison
01:43:55 building out a neural net library (multi-layer perceptron) in micrograd
01:51:04 creating a tiny dataset, writing the loss function
01:57:56 collecting all of the parameters of the neural net
02:01:12 doing gradient descent optimization manually, training the network
02:14:03 summary of what we learned, how to go towards modern neural nets
02:16:46 walkthrough of the full code of micrograd on github
02:21:10 real stuff: diving into PyTorch, finding their backward pass for tanh
02:24:39 conclusion
02:25:20 outtakes :)
